{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moamt2aNOWIC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction to Sentiment Analysis**:\n",
        "Sentiment analysis, also known as opinion mining, is a subfield of natural language processing (NLP) that aims to determine the sentiment or subjective attitude expressed in a piece of text. It involves analyzing and categorizing text as positive, negative, or neutral based on the underlying sentiment conveyed by the author.\n",
        "\n",
        "Sentiment analysis has various applications, including understanding customer opinions, social media monitoring, brand reputation management, market research, and more. By automating the process of sentiment analysis, businesses and organizations can gain valuable insights from large volumes of text data.\n",
        "\n",
        "Introduction to the \"IMDB Movie Review\" Dataset:\n",
        "The \"IMDB Movie Review\" dataset is a widely used dataset for sentiment analysis tasks. It contains a collection of movie reviews from the Internet Movie Database (IMDB) website, where each review is labeled with a sentiment (positive or negative) based on the overall sentiment expressed in the review. The dataset is balanced, meaning it contains an equal number of positive and negative reviews."
      ],
      "metadata": {
        "id": "xjaw540-Oy9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset I have chosen is the \"Twitter Sentiment Analysis Dataset.\" This dataset focuses on sentiment analysis of tweets, providing a valuable resource for understanding and predicting public sentiment on various topics. It is particularly useful for analyzing real-time opinions and sentiments shared on Twitter, a popular social media platform.\n",
        "\n",
        "The Twitter Sentiment Analysis Dataset comprises a collection of tweets labeled with sentiment values such as positive, negative, or neutral. The dataset captures a wide range of subjects, including politics, entertainment, technology, and more, making it versatile for sentiment analysis tasks across different domains."
      ],
      "metadata": {
        "id": "Al1DdOyyQNI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repository: Sentiment140\n",
        "Link: https://www.kaggle.com/kazanova/sentiment140\n",
        "\n",
        "The Sentiment140 repository on Kaggle provides a dataset containing 1.6 million tweets labeled with sentiment values (0 for negative sentiment and 4 for positive sentiment). This dataset is commonly used for sentiment analysis tasks and can be easily accessed and downloaded from the Kaggle website."
      ],
      "metadata": {
        "id": "VnB1EBVZQ3dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv5rndRZRNUS",
        "outputId": "f0242987-4440-41ea-e32e-f9abd2301d87"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create the .kaggle directory in Colab\n",
        "!mkdir -p ~/.kaggle\n"
      ],
      "metadata": {
        "id": "Sd0e-kpPZR6_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/kaggle.json' ~/.kaggle/\n"
      ],
      "metadata": {
        "id": "XTYsA5TSak8E"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new folder named \"Datasets\" in my drive\n",
        "!mkdir '/content/drive/MyDrive/Datasets'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4CzOzdxSDkB",
        "outputId": "04dbb814-618f-4e3d-e082-4b61b348f757"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/Datasets’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Navigate to the newly created folder\n",
        "%cd /content/drive/MyDrive/Datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-jXw4slU0Dn",
        "outputId": "8675a81c-77fb-4146-a01a-3ff986debbe1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Datasets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the !kaggle command to download the dataset directly from the Kaggle repository.\n",
        "!kaggle datasets download -d kazanova/sentiment140\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9HTMeG0U-3o",
        "outputId": "b5fb22dd-3919-4021-cf3e-f63450d2a888"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "sentiment140.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract the contents using the\n",
        "!unzip sentiment140.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm7mxhmpVMTZ",
        "outputId": "8316d50b-ddcf-431c-9214-fd6a653d0d00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sentiment140.zip\n",
            "replace training.1600000.processed.noemoticon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "X4cHvE-TbZ4f"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the CSV file into a pandas DataFrame using the read_csv() function:"
      ],
      "metadata": {
        "id": "9dtrZKSLdE8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1', header=None)\n"
      ],
      "metadata": {
        "id": "Lm1WUWrZc-O2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICo9PO9tdJHI",
        "outputId": "e37606f5-d8fe-4166-b2f8-2116ebed5fe1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         0           1                             2         3  \\\n",
            "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
            "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
            "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
            "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
            "...     ..         ...                           ...       ...   \n",
            "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
            "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
            "\n",
            "                       4                                                  5  \n",
            "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
            "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
            "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
            "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
            "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
            "...                  ...                                                ...  \n",
            "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
            "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
            "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
            "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
            "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
            "\n",
            "[1600000 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P6TKto2dUfv",
        "outputId": "0d4a1e2c-e4be-4596-b53c-97db94bcf45d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 6 columns):\n",
            " #   Column  Non-Null Count    Dtype \n",
            "---  ------  --------------    ----- \n",
            " 0   0       1600000 non-null  int64 \n",
            " 1   1       1600000 non-null  int64 \n",
            " 2   2       1600000 non-null  object\n",
            " 3   3       1600000 non-null  object\n",
            " 4   4       1600000 non-null  object\n",
            " 5   5       1600000 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 73.2+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column3_data = data[3]\n",
        "column3_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezvaYBe2eD39",
        "outputId": "260afcee-404e-4325-fb0b-a5ad950a93f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          NO_QUERY\n",
              "1          NO_QUERY\n",
              "2          NO_QUERY\n",
              "3          NO_QUERY\n",
              "4          NO_QUERY\n",
              "             ...   \n",
              "1599995    NO_QUERY\n",
              "1599996    NO_QUERY\n",
              "1599997    NO_QUERY\n",
              "1599998    NO_QUERY\n",
              "1599999    NO_QUERY\n",
              "Name: 3, Length: 1600000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = data[3].unique()\n",
        "print(unique_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8DWxZl5eeal",
        "outputId": "78fdabd5-c5d6-4a3b-9f98-c761db1f351e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NO_QUERY']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This column does not contain any useful information for sentiment analysis."
      ],
      "metadata": {
        "id": "dd3rzl8JgbUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.drop(3, axis=1)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "oJ0SCGwsgKpU",
        "outputId": "5cc86fa9-89e0-44c9-c3d2-d7f113706e16"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0           1                             2                4  \\\n",
              "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
              "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
              "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
              "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
              "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
              "...     ..         ...                           ...              ...   \n",
              "1599995  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
              "1599996  4  2193601969  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
              "1599997  4  2193601991  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
              "1599998  4  2193602064  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
              "1599999  4  2193602129  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
              "\n",
              "                                                         5  \n",
              "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
              "1        is upset that he can't update his Facebook by ...  \n",
              "2        @Kenichan I dived many times for the ball. Man...  \n",
              "3          my whole body feels itchy and like its on fire   \n",
              "4        @nationwideclass no, it's not behaving at all....  \n",
              "...                                                    ...  \n",
              "1599995  Just woke up. Having no school is the best fee...  \n",
              "1599996  TheWDB.com - Very cool to hear old Walt interv...  \n",
              "1599997  Are you ready for your MoJo Makeover? Ask me f...  \n",
              "1599998  Happy 38th Birthday to my boo of alll time!!! ...  \n",
              "1599999  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
              "\n",
              "[1600000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca81610d-9a0c-47f9-aa1f-db5c9a0e29a9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>_TheSpecialOne_</td>\n",
              "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he can't update his Facebook by ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811184</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>ElleCTF</td>\n",
              "      <td>my whole body feels itchy and like its on fire</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1467811193</td>\n",
              "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
              "      <td>Karoli</td>\n",
              "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601966</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>AmandaMarie1028</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601969</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>TheWDBoards</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>2193601991</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>bpbabe</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602064</td>\n",
              "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
              "      <td>tinydiamondz</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>2193602129</td>\n",
              "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
              "      <td>RyanTrevMorris</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600000 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca81610d-9a0c-47f9-aa1f-db5c9a0e29a9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca81610d-9a0c-47f9-aa1f-db5c9a0e29a9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca81610d-9a0c-47f9-aa1f-db5c9a0e29a9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Column 0: This column contains the sentiment label, where 0 represents a negative sentiment and 4 represents a positive sentiment.\n",
        "- Column 1: It contains a numeric timestamp associated with each tweet.\n",
        "- Column 2: This column contains the timestamp of the tweet in the format \"Day Month DD HH:MM:SS PDT YYYY.\"\n",
        "- Column 4: It represents the username or handle of the Twitter account that posted the tweet.\n",
        "- Column 5: This column contains the actual text of the tweet.\n",
        "\n",
        "By analyzing the tweet text and the associated sentiment labels, we can perform sentiment analysis tasks to predict the sentiment of the given text."
      ],
      "metadata": {
        "id": "smG08lWvhTUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Cleaning:\n",
        "•\tRemove special characters, punctuation, URLs, and mentions from the tweet text. This step helps eliminate noise and irrelevant information that may not contribute to sentiment analysis.\n",
        "\n",
        "•\tConvert the text to lowercase to ensure consistent analysis and avoid distinguishing between words based on case sensitivity.\n",
        "with their codes"
      ],
      "metadata": {
        "id": "sXD11xdoi57k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # provides regular expression matching operations\n",
        "\n",
        "def clean_text(text):\n",
        "    # Removes any characters that are not alphabets, digits, or whitespace\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "\n",
        "    # Remove mentions\n",
        "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
        "\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "wWsUmv-Mgf2n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the 'clean_text()' function to both columns 4 and 5\n",
        "data[4] = data[4].apply(clean_text)  # Clean column 4\n",
        "data[5] = data[5].apply(clean_text)  # Clean column 5\n",
        "\n",
        "#data = data.drop(5, axis=1)\n"
      ],
      "metadata": {
        "id": "9zf4ffqDkCO8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "vsBCFn-EkuWX",
        "outputId": "193cd9ec-c839-4ad6-dbdf-f344bff267c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0           1                             2              4  \\\n",
              "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  thespecialone   \n",
              "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
              "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
              "\n",
              "                                                   5  \n",
              "0  switchfoot   awww thats a bummer  you shoulda ...  \n",
              "1  is upset that he cant update his facebook by t...  \n",
              "2  kenichan i dived many times for the ball manag...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83e584ed-ee48-482e-b676-deaeaaac9952\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>thespecialone</td>\n",
              "      <td>switchfoot   awww thats a bummer  you shoulda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he cant update his facebook by t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>kenichan i dived many times for the ball manag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83e584ed-ee48-482e-b676-deaeaaac9952')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83e584ed-ee48-482e-b676-deaeaaac9952 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83e584ed-ee48-482e-b676-deaeaaac9952');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns={0: 'Sentiment', 1: 'Timestamp', 2: 'Date', 4: 'Username', 5: 'Text'})\n"
      ],
      "metadata": {
        "id": "kQzzHCM5of3F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "pLs0tlNLogmP",
        "outputId": "e987a99d-f286-4955-849e-a2471df83a8c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sentiment   Timestamp                          Date       Username  \\\n",
              "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  thespecialone   \n",
              "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  scotthamilton   \n",
              "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009       mattycus   \n",
              "\n",
              "                                                Text  \n",
              "0  switchfoot   awww thats a bummer  you shoulda ...  \n",
              "1  is upset that he cant update his facebook by t...  \n",
              "2  kenichan i dived many times for the ball manag...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5b1e2f6-5c65-4362-8387-f0bbd2cac325\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Date</th>\n",
              "      <th>Username</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810369</td>\n",
              "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
              "      <td>thespecialone</td>\n",
              "      <td>switchfoot   awww thats a bummer  you shoulda ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810672</td>\n",
              "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
              "      <td>scotthamilton</td>\n",
              "      <td>is upset that he cant update his facebook by t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1467810917</td>\n",
              "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
              "      <td>mattycus</td>\n",
              "      <td>kenichan i dived many times for the ball manag...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5b1e2f6-5c65-4362-8387-f0bbd2cac325')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5b1e2f6-5c65-4362-8387-f0bbd2cac325 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5b1e2f6-5c65-4362-8387-f0bbd2cac325');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword Removal:\n",
        "Stopword removal helps reduce noise and focus on more meaningful words.\n",
        "Remove common words, known as stopwords, that appear frequently but carry little sentiment-related information (e.g., \"the,\" \"is,\" \"and\").\n"
      ],
      "metadata": {
        "id": "Nqj3WCqopKtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')  # Download stopwords data (needed once)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))  # Set of English stopwords\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "data['Text'] = data['Text'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXGPQ8ksokv1",
        "outputId": "3e61c47b-8db7-4397-efa9-10b8a39aa342"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function takes a `text` string as input, splits it into individual `words`, applies the `stopword` removal using a list comprehension, and then joins the `filtered_words` back into a string."
      ],
      "metadata": {
        "id": "1AIuf2KosRdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming or Lemmatization:\n",
        "- Perform stemming or lemmatization to reduce words to their base or root forms.\n",
        "- Stemming reduces words to their stems, which may not always be actual English words but are consistent representations (e.g., \"running\" becomes \"run\").\n",
        "- Lemmatization maps words to their dictionary form or lemma (e.g., \"running\" becomes \"run\"). Lemmatization retains actual English words but may result in fewer dimensions compared to stemming.\n",
        "\n",
        "Advantages:\n",
        "- help reduce the dimensionality of the data and simplify the analysis\n",
        "- Contextual Understanding\n",
        "- Domain-Specific Terms: Consider whether your dataset contains domain-specific terms or jargon that may not be handled well by stemming or lemmatization. In such cases, it may be better to skip these steps to preserve the specific vocabulary of the domain.\n",
        "- Time and Resources: Stemming and lemmatization can add computational overhead, especially for large datasets. If you have limited computational resources or time constraints, you might choose to skip these steps."
      ],
      "metadata": {
        "id": "NOHqd8bhuGbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtzZMAFAvkcZ",
        "outputId": "22fa82a0-b8bc-447d-913d-fa6e3efde130"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def perform_stemming(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(token) for token in tokens]\n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "def perform_lemmatization(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(lemmatized_words)\n",
        "\n",
        "data['Stemmed_Text'] = data['Text'].apply(perform_stemming)\n",
        "data['Lemmatized_Text'] = data['Text'].apply(perform_lemmatization)\n"
      ],
      "metadata": {
        "id": "QUbHn1DOtrKZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extraction:\n",
        "Convert the preprocessed text into numerical representations that machine learning models can process.\n",
        "\n",
        "**Bag-of-Words (BoW)**: In the bag-of-words approach, each word is treated as a separate feature, and the presence or absence of words is used to represent the text. This technique does not consider the order or context of the words, but it can be effective in capturing important keywords or themes in the text.\n",
        "\n",
        "To apply the **Bag-of-Words (BoW)** technique to the dataset, you can use the `CountVectorize`r class from the `sklearn.feature_extraction.text` module in `scikit-learn`. This class allows you to convert a collection of text documents into a matrix of token counts."
      ],
      "metadata": {
        "id": "kKJ1CplI2S0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to your text data\n",
        "bow_matrix = vectorizer.fit_transform(data['Text'])\n",
        "\n",
        "# Print the shape of the BoW matrix\n",
        "print(\"Shape of BoW matrix:\", bow_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQYI83up3aXj",
        "outputId": "89379235-bb52-4fcc-ca39-31db834498f7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of BoW matrix: (1600000, 772770)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 1,600,000 documents (rows) and 772,770 unique words (columns) in your dataset. Each entry in the matrix represents the count of a specific word in a particular document.\n",
        "\n",
        "The large number of unique words suggests that your dataset contains a wide vocabulary, which can provide rich information for analysis. However, it's important to consider the computational resources required to work with such a large matrix."
      ],
      "metadata": {
        "id": "adPvax6K6Ft5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train-Test Split:\n",
        "- Split the preprocessed dataset into training and testing subsets.\n",
        "- The training subset is used to train the sentiment analysis model, while the testing subset is used to evaluate its performance.\n",
        "- A common split is to allocate around 80% of the data for training and the remaining 20% for testing.\n"
      ],
      "metadata": {
        "id": "VEaaPZJ47Goa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow_matrix, data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing data\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY0sA0xZ6It0",
        "outputId": "44055323-565a-4c3b-8dbd-460188c696c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (1280000, 772770)\n",
            "Shape of X_test: (320000, 772770)\n",
            "Shape of y_train: (1280000,)\n",
            "Shape of y_test: (320000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training and Evaluation\n",
        "1- Evaluating a logistic regression model using scikit-learn use the `data['Stemmed_Text']`"
      ],
      "metadata": {
        "id": "MnamQUd578yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Stemmed_Text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the vectorizer to your text data\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Create an instance of the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter for convergence\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Update the sentiment labels to 0 and 1\n",
        "y_test[y_test == 4] = 1\n",
        "y_pred[y_pred == 4] = 1\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF1VwbN07cja",
        "outputId": "496c1dbb-c46f-4046-8a67-f0fbb2ac1072"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.781025\n",
            "Precision: 0.7729769867909493\n",
            "Recall: 0.7977209574719948\n",
            "F1 Score: 0.7851540702130921\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Evaluating a logistic regression model using scikit-learn use the `data['Lemmatized_Text']`"
      ],
      "metadata": {
        "id": "YuDs2YeXBEFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Lemmatized_Text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the vectorizer to your text data\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Create an instance of the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter for convergence\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Update the sentiment labels to 0 and 1\n",
        "y_test[y_test == 4] = 1\n",
        "y_pred[y_pred == 4] = 1\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Ly-_n8__MR",
        "outputId": "ba544383-b5ff-4e77-c4fc-e09c11f69801"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.784525\n",
            "Precision: 0.7761476744887494\n",
            "Recall: 0.8016024323078265\n",
            "F1 Score: 0.7886697152104353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3- modified code using the data['Lemmatized_Text'] column for the **Naive Bayes classifier**"
      ],
      "metadata": {
        "id": "Fw2WHGEEByLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Split the dataset into training and testing subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Lemmatized_Text'], data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the vectorizer to your text data\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Create an instance of the Naive Bayes classifier\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Update the sentiment labels to 0 and 1\n",
        "y_test[y_test == 4] = 1\n",
        "y_pred[y_pred == 4] = 1\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label=1)\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0br6idArBxmK",
        "outputId": "392f4f28-8131-4c6a-fb0d-384b5ae417d2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.77321875\n",
            "Precision: 0.7923209892959244\n",
            "Recall: 0.7424831470474624\n",
            "F1 Score: 0.7665929061225539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `accuracy` represents the overall correctness of the predictions, while `precision` measures the proportion of correctly predicted positive sentiment instances out of all predicted positive instances. `Recall` indicates the proportion of correctly predicted positive sentiment instances out of all actual positive instances. The `F1 score` is the harmonic mean of precision and recall, providing a balanced measure of the model's performance.\n",
        "\n",
        "Based on these results, we can see that both models achieve relatively similar performance in terms of accuracy and F1 score. However, Naive Bayes shows slightly higher precision while Logistic Regression has a slightly higher recall."
      ],
      "metadata": {
        "id": "8WHoKT30_slq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lower accuracy, precision, recall, and F1 score indicate that the classifier is struggling to accurately classify the sentiment of the movie reviews. This could be due to the limitations of the Naive Bayes algorithm, which assumes independence between features and may not capture complex relationships in the text data."
      ],
      "metadata": {
        "id": "jNOS32lBBR70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Optimization and Fine-tuning\n",
        "Improve the model's performance by using use cross-validation, I utilize a technique called **\"memory-friendly cross-validation\"** or **\"incremental cross-validation.\"** This approach allows to perform cross-validation without loading the entire dataset into memory at once.\n",
        "\n",
        "### Optimize and fine-tune the Naive Bayes model using cross-validation"
      ],
      "metadata": {
        "id": "0c1cx4e4xx-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ],
      "metadata": {
        "id": "9hv3exeTDq5p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Naive Bayes classifier\n",
        "naive_bayes = MultinomialNB()"
      ],
      "metadata": {
        "id": "lLsXkAZZDvYx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform cross-validation. Here X_train represents the training data containing the lemmatized text, and y_train contains the corresponding sentiment labels\n",
        "scores = cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n"
      ],
      "metadata": {
        "id": "wLuntgvhD0dQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the mean accuracy and other evaluation metrics\n",
        "mean_accuracy = scores.mean()\n",
        "precision = cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring='precision').mean()\n",
        "recall = cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring='recall').mean()\n",
        "f1 = cross_val_score(naive_bayes, X_train, y_train, cv=5, scoring='f1').mean()\n"
      ],
      "metadata": {
        "id": "GZqgH-RNFMBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Create a pipeline with CountVectorizer and Naive Bayes classifier\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', CountVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(pipeline, data['Lemmatized_Text'], data['Sentiment'], cv=5, scoring='accuracy')\n",
        "\n",
        "# Compute the mean accuracy and print the results\n",
        "mean_accuracy = cv_scores.mean()\n",
        "print(\"Mean Accuracy:\", mean_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0NNSqOvIh9b",
        "outputId": "57c01c7f-b60f-4172-9c73-613018ae4b49"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.7620150000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Prediction:\n",
        "- Once we have a trained and optimized model, use it to predict the sentiment of new, unseen tweets or text data.\n",
        "•\tPass the preprocessed input text through the trained model to obtain sentiment predictions (e.g., positive or negative sentiment).\n"
      ],
      "metadata": {
        "id": "oTcyqu7eKZij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Perform stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "    # Join the stemmed words back into a single string\n",
        "    preprocessed_text = ' '.join(stemmed_words)\n",
        "\n",
        "    return preprocessed_text\n"
      ],
      "metadata": {
        "id": "JT5HmK-rKkvD"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Specify the new text for sentiment prediction\n",
        "new_text = \"This is a great movie. I loved it!\"\n",
        "\n",
        "# Preprocess the input text (e.g., remove stopwords, perform stemming/lemmatization)\n",
        "preprocessed_text = preprocess_text(new_text)\n",
        "\n",
        "# Vectorize the preprocessed text using the same vectorizer used during training\n",
        "text_vector = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Make sentiment prediction using the trained model\n",
        "sentiment_prediction = model.predict(text_vector)\n",
        "\n",
        "# Print the sentiment prediction\n",
        "if sentiment_prediction == 0:\n",
        "    print(\"Negative sentiment\")\n",
        "else:\n",
        "    print(\"Positive sentiment\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP89DvhzLOnD",
        "outputId": "4f6d29b0-2017-48e6-d937-70828eb439f6"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "To predict the sentiment of a new text using the trained model, follow these steps:\n",
        "\n",
        "Preprocess the new text in a similar manner as the training data, including steps like removing stopwords, performing stemming/lemmatization, and any other necessary text normalization techniques.\n",
        "\n",
        "Vectorize the preprocessed text using the same vectorizer used during training. This step converts the text into a numerical representation that can be understood by the model.\n",
        "\n",
        "Use the trained model to make a sentiment prediction on the vectorized text. This can be done by calling the predict method of the model and passing the vectorized text as input."
      ],
      "metadata": {
        "id": "5XDAWR-YOBrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text = \"I hate this movie, it has a derogatory scene!\""
      ],
      "metadata": {
        "id": "WODKXQR7OXH9"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the input text (e.g., remove stopwords, perform stemming/lemmatization)\n",
        "preprocessed_text = preprocess_text(new_text)\n",
        "\n",
        "# Vectorize the preprocessed text using the same vectorizer used during training\n",
        "text_vector = vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Make sentiment prediction using the trained model\n",
        "sentiment_prediction = model.predict(text_vector)\n",
        "\n",
        "# Print the sentiment prediction\n",
        "if sentiment_prediction == 0:\n",
        "    print(\"Negative sentiment\")\n",
        "else:\n",
        "    print(\"Positive sentiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bpyph070OrPc",
        "outputId": "569992a4-d487-4ff7-f19a-71100cba8f1f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative sentiment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOwSsdrTO8bi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}