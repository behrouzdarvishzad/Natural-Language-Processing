{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I aimed to build a spam detection system using natural language processing techniques. We started by loading a dataset containing SMS messages labeled as spam or ham (not spam). The dataset was preprocessed to remove unnecessary characters, convert text to lowercase, tokenize the text, and remove stopwords.\n",
        "\n",
        "Next, we applied TF-IDF vectorization to convert the tokenized text into numerical features. TF-IDF assigns weights to words based on their frequency in the document and across the dataset. This allowed us to represent the text data as a matrix of numerical values.\n",
        "\n",
        "We then split the dataset into training and testing sets for model evaluation. We chose to use the Support Vector Machine (SVM) algorithm for classification. The SVM model was trained on the TF-IDF features of the training set.\n",
        "\n",
        "After training the SVM model, we evaluated its performance using various evaluation metrics such as accuracy, precision, recall, F1-score, and ROC AUC. These metrics provide insights into how well the model performs in distinguishing between spam and ham messages.\n",
        "\n",
        "The evaluation results showed that the SVM model achieved high accuracy, precision, and F1-score, indicating its effectiveness in spam detection. The ROC AUC score demonstrated the model's ability to discriminate between positive and negative classes.\n",
        "\n",
        "To test the model, we provided an example text message, preprocessed it using the same steps as the training data, applied TF-IDF vectorization, and made a prediction using the SVM model. This allowed us to classify the example text as either spam or ham.\n",
        "\n",
        "Overall, this project demonstrated the process of building a spam detection system using NLP techniques, including preprocessing, feature extraction using TF-IDF, model training with SVM, evaluation of model performance, and testing with example data."
      ],
      "metadata": {
        "id": "YxeKAAVMSCAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
        "\n",
        "The dataset used in the spam detection code is called the \"SMS Spam Collection Dataset.\" It is a collection of SMS messages labeled as spam or ham (not spam). The dataset contains a total of 5,572 messages.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AlR5zXB7SjNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame using the 'latin-1' encoding\n",
        "data = pd.read_csv(csv_path, encoding='latin-1')\n",
        "\n",
        "# Explore the loaded data\n",
        "print(data.head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IhGXvf8_sFF",
        "outputId": "0630bcc6-1201-4ffe-974d-503b945a78ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2 Unnamed: 2  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
            "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
            "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4  \n",
            "0        NaN        NaN  \n",
            "1        NaN        NaN  \n",
            "2        NaN        NaN  \n",
            "3        NaN        NaN  \n",
            "4        NaN        NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvlmUvZ7A6Lz",
        "outputId": "ebaecb5c-a245-4e11-81ba-3b7b33a5a8de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5572 entries, 0 to 5571\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   v1          5572 non-null   object\n",
            " 1   v2          5572 non-null   object\n",
            " 2   Unnamed: 2  50 non-null     object\n",
            " 3   Unnamed: 3  12 non-null     object\n",
            " 4   Unnamed: 4  6 non-null      object\n",
            "dtypes: object(5)\n",
            "memory usage: 217.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the columns with mostly null values\n",
        "data = data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1)\n",
        "\n",
        "# Explore the loaded data\n",
        "print(data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLZJibonBR9B",
        "outputId": "f45a7a19-218a-418b-b1d3-97a27ec9e3da"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns={\"v1\": \"Label\", \"v2\": \"Text\"})\n"
      ],
      "metadata": {
        "id": "6lQotGdQBmE6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['Label'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CYb8s-YBxsb",
        "outputId": "e961954e-6a97-407c-ec61-d8a8553a54a8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['ham', 'spam'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing:\n",
        "Preprocess the text data by removing unnecessary characters, converting to lowercase, and removing stopwords."
      ],
      "metadata": {
        "id": "wxqhXleCA0hE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords data (needed once)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Remove unnecessary characters and convert to lowercase\n",
        "data['Text'] = data['Text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x.lower()))\n",
        "\n",
        "# Tokenize the text\n",
        "data['Tokenized_Text'] = data['Text'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['Tokenized_Text'] = data['Tokenized_Text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Lowercase normalization\n",
        "data['Tokenized_Text'] = data['Tokenized_Text'].apply(lambda x: [word.lower() for word in x])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbVDPLBjCr4a",
        "outputId": "5d2f8ae5-68ec-4427-bec6-a5752eb2e628"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction\n",
        "First convert the tokenized text back into strings by joining the tokens using a space separator. Then, we initialize a TF-IDF vectorizer object. We apply the TF-IDF vectorization on the processed text using the fit_transform() method, which returns the feature matrix X representing the TF-IDF values for each document."
      ],
      "metadata": {
        "id": "un0RzqpCGGJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert tokenized text to strings\n",
        "data['processed_text'] = data['Tokenized_Text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Apply TF-IDF vectorization on the processed text\n",
        "X = tfidf_vectorizer.fit_transform(data['processed_text'])\n",
        "\n",
        "# Print the shape of the feature matrix\n",
        "print(\"Shape of X:\", X.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P08b27ruGYv7",
        "outputId": "dbb939e1-da32-4e07-9d65-059ad8e21a99"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (5572, 9314)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that we have 5572 documents (text messages) and 9314 unique features (words) represented by their TF-IDF values. Each element in the matrix corresponds to the TF-IDF value of a specific word in a specific document."
      ],
      "metadata": {
        "id": "exQfz8IPGyR9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Text"
      ],
      "metadata": {
        "id": "znzyk71AF22a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and the target variable (y)\n",
        "X = data['Text']\n",
        "y = data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the resulting sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbsyCIMVHI5e",
        "outputId": "8051c3c4-0650-4184-f8fe-3b8813b00630"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (4457,)\n",
            "X_test shape: (1115,)\n",
            "y_train shape: (4457,)\n",
            "y_test shape: (1115,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM for modeling and evaluation\n",
        "Support Vector Machine used for classification and regression tasks. It is a popular and powerful algorithm known for its effectiveness in handling complex datasets. The main idea behind SVM is to find an optimal hyperplane that separates different classes in the data space.\n",
        "\n",
        "<img src = \"https://www.researchgate.net/profile/Ismail-Calikusu/publication/338698374/figure/fig3/AS:849434183233538@1579532299528/Optimal-Hyperplane-and-Margin-of-SVM.png\">\n",
        "\n",
        "In binary classification, the hyperplane aims to maximize the margin between the two classes, which is the distance between the hyperplane and the nearest data points of each class. These data points are called support vectors."
      ],
      "metadata": {
        "id": "VluzdAiHIjHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF vectorization\n",
        "Term Frequency-Inverse Document Frequency is a numerical statistic used to evaluate the importance of a term (word) in a document within a collection or corpus.\n",
        "\n",
        "In the context of text classification, TF-IDF vectorization converts a collection of text documents into a matrix representation where each row represents a document and each column represents a unique term in the corpus. The values in the matrix correspond to the TF-IDF score of each term in each document.\n",
        "\n",
        "By applying TF-IDF vectorization, we transform the textual data into numerical features that can be used as input for machine learning algorithms, such as SVM, to train and classify text data."
      ],
      "metadata": {
        "id": "AuWxNAcnNSKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Apply TF-IDF vectorization on the processed text\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X = tfidf_vectorizer.fit_transform(data['processed_text'])\n",
        "y = data['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-jgoiCtMy9_",
        "outputId": "b7319363-7104-4e09-bccd-43682308ccad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9668161434977578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm.predict(X_test)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, pos_label='spam')\n",
        "recall = recall_score(y_test, y_pred, pos_label='spam')\n",
        "f1 = f1_score(y_test, y_pred, pos_label='spam')\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "y_pred_encoded = label_encoder.transform(y_pred)\n",
        "\n",
        "# Compute ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test_encoded, y_pred_encoded)\n",
        "\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-Score:\", f1)\n",
        "print(\"ROC AUC:\", roc_auc)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewWUB-eBOsbd",
        "outputId": "5863d1b1-6d0d-4852-e1c5-044d85986848"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9668161434977578\n",
            "Precision: 0.9829059829059829\n",
            "Recall: 0.7666666666666667\n",
            "F1-Score: 0.8614232209737828\n",
            "ROC AUC: 0.8822970639032814\n",
            "Confusion Matrix:\n",
            "[[963   2]\n",
            " [ 35 115]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98       965\n",
            "        spam       0.98      0.77      0.86       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.97      0.88      0.92      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: The accuracy of the model is 0.9668, which indicates that it correctly classifies 96.68% of the instances in the test set.\n",
        "\n",
        "Precision: Precision is a measure of the model's ability to correctly identify spam messages. A precision score of 0.9829 suggests that when the model predicts a message as spam, it is correct 98.29% of the time.\n",
        "\n",
        "Recall: Recall, also known as sensitivity or true positive rate, measures the model's ability to correctly detect spam messages. A recall score of 0.7667 means that the model identifies 76.67% of the actual spam messages.\n",
        "\n",
        "F1-Score: The F1-score is the harmonic mean of precision and recall. It provides a balanced measure of the model's performance. The F1-score of 0.8614 indicates a good trade-off between precision and recall.\n",
        "\n",
        "ROC AUC: The ROC AUC (Receiver Operating Characteristic Area Under the Curve) score measures the model's ability to discriminate between classes. A value of 0.8823 suggests that the model performs well in distinguishing between spam and non-spam messages.\n",
        "\n",
        "Confusion Matrix: The confusion matrix shows the number of true positive (spam correctly classified), true negative (ham correctly classified), false positive (ham misclassified as spam), and false negative (spam misclassified as ham) predictions. In this case, the model correctly classified 963 ham messages and 115 spam messages, but misclassified 35 spam messages as ham and 2 ham messages as spam.\n",
        "\n",
        "Classification Report: The classification report provides a summary of precision, recall, and F1-score for both classes (ham and spam), along with the support (number of instances) for each class. It gives a comprehensive view of the model's performance on a class-by-class basis.\n",
        "\n",
        "Overall, the SVM model shows promising performance with high accuracy and precision. However, it has a relatively lower recall, indicating that there is room for improvement in detecting more spam messages correctly."
      ],
      "metadata": {
        "id": "g90uXif-QD3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the SVM model with an example"
      ],
      "metadata": {
        "id": "-zRQ8I0MRMae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Function to preprocess the text\n",
        "def preprocess_text(text):\n",
        "    # Remove unnecessary characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = ' '.join(tokens)\n",
        "\n",
        "    return processed_text\n",
        "\n",
        "# Example text message\n",
        "example_text = \"Congratulations! You have won a free vacation. Reply now to claim your prize.\"\n",
        "\n",
        "# Preprocess the example text\n",
        "example_text = preprocess_text(example_text)\n",
        "\n",
        "# Apply TF-IDF vectorization on the example text\n",
        "example_tfidf = tfidf_vectorizer.transform([example_text])\n",
        "\n",
        "# Make a prediction using the SVM model\n",
        "prediction = svm.predict(example_tfidf)\n",
        "\n",
        "# Print the predicted label\n",
        "print(\"Predicted Label:\", prediction[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIoFQ0u_QrYm",
        "outputId": "7a5c54f0-7312-4359-fa95-2950aa0deeb8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: spam\n"
          ]
        }
      ]
    }
  ]
}